{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bb0e550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/16 17:06:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Q1.\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_session = SparkSession.builder.master(\"local[*]\").appName(\"homework\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e6c21d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfed44e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07cf7bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2\n",
    "\n",
    "URL = \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-06.csv.gz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3d7036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've ran these below 2 commands in my terminal to get data in my desired location\n",
    "#1.  mkdir -p data/raw/fhvhv/2021/06\n",
    "#2. wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-06.csv.gz -O data/raw/fhvhv/2021/06/fhvhv_tripdata_2021_06.csv.gz\n",
    "\n",
    "# TODO:\n",
    "# we need to write a pyspark code to read csv data from URL or atleast save data from URL to desired local location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "663c029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhvhv = spark_session.read.option(\"header\", \"true\").csv(\"../notebooks/data/raw/fhvhv/2021/06/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1621f488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhvhv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9aff80f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see that the schema has all strings, so we need to read using pandas to get correct spark alike schema and then \n",
    "# use that schema to read data into spark DF again\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd_df = pd.read_csv('../notebooks/data/raw/fhvhv/2021/06/fhvhv_tripdata_2021_06.csv.gz', nrows=100) #as we just need schema, we can even read 1 or 10 rows as well\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "036a1080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dispatching_base_num      object\n",
       "pickup_datetime           object\n",
       "dropoff_datetime          object\n",
       "PULocationID               int64\n",
       "DOLocationID               int64\n",
       "SR_Flag                   object\n",
       "Affiliated_base_number    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7c189aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>SR_Flag</th>\n",
       "      <th>Affiliated_base_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B02764</td>\n",
       "      <td>2021-06-01 00:02:41</td>\n",
       "      <td>2021-06-01 00:07:46</td>\n",
       "      <td>174</td>\n",
       "      <td>18</td>\n",
       "      <td>N</td>\n",
       "      <td>B02764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B02764</td>\n",
       "      <td>2021-06-01 00:16:16</td>\n",
       "      <td>2021-06-01 00:21:14</td>\n",
       "      <td>32</td>\n",
       "      <td>254</td>\n",
       "      <td>N</td>\n",
       "      <td>B02764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B02764</td>\n",
       "      <td>2021-06-01 00:27:01</td>\n",
       "      <td>2021-06-01 00:42:11</td>\n",
       "      <td>240</td>\n",
       "      <td>127</td>\n",
       "      <td>N</td>\n",
       "      <td>B02764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B02764</td>\n",
       "      <td>2021-06-01 00:46:08</td>\n",
       "      <td>2021-06-01 00:53:45</td>\n",
       "      <td>127</td>\n",
       "      <td>235</td>\n",
       "      <td>N</td>\n",
       "      <td>B02764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B02510</td>\n",
       "      <td>2021-06-01 00:45:42</td>\n",
       "      <td>2021-06-01 01:03:33</td>\n",
       "      <td>144</td>\n",
       "      <td>146</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dispatching_base_num      pickup_datetime     dropoff_datetime  \\\n",
       "0               B02764  2021-06-01 00:02:41  2021-06-01 00:07:46   \n",
       "1               B02764  2021-06-01 00:16:16  2021-06-01 00:21:14   \n",
       "2               B02764  2021-06-01 00:27:01  2021-06-01 00:42:11   \n",
       "3               B02764  2021-06-01 00:46:08  2021-06-01 00:53:45   \n",
       "4               B02510  2021-06-01 00:45:42  2021-06-01 01:03:33   \n",
       "\n",
       "   PULocationID  DOLocationID SR_Flag Affiliated_base_number  \n",
       "0           174            18       N                 B02764  \n",
       "1            32           254       N                 B02764  \n",
       "2           240           127       N                 B02764  \n",
       "3           127           235       N                 B02764  \n",
       "4           144           146       N                    NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aec36c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jagadish/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/home/jagadish/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "field Affiliated_base_number: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2042/3738516286.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_pandas\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0;31m# Create a DataFrame from pandas DataFrame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             return super(SparkSession, self).createDataFrame(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m    892\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             )\n",
      "\u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    435\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0mconverted_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_from_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimezone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     def _convert_from_pandas(\n",
      "\u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchemaFromList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mtupled_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0minfer_dict_as_struct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferDictAsStruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0mprefer_timestamp_ntz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_timestamp_ntz_preferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         schema = reduce(\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfer_dict_as_struct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefer_timestamp_ntz\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m         \u001b[0mnfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m         fields = [\n\u001b[0m\u001b[1;32m   1384\u001b[0m             StructField(\n\u001b[1;32m   1385\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_merge_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNullType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1383\u001b[0m         fields = [\n\u001b[1;32m   1384\u001b[0m             StructField(\n\u001b[0;32m-> 1385\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_merge_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNullType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1386\u001b[0m             )\n\u001b[1;32m   1387\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1376\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         \u001b[0;31m# TODO: type cast (such as int -> long)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not merge type %s and %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m     \u001b[0;31m# same type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: field Affiliated_base_number: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>"
     ]
    }
   ],
   "source": [
    "spark_session.createDataFrame(pd_df).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "356db014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cebe1ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seems like we have some NaN values in the Affiliated_base_number column\n",
    "len(pd_df[\"Affiliated_base_number\"].iloc[np.where(pd_df[\"Affiliated_base_number\"].isna() == True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80cfa282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2042/2845800174.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pd_df[\"Affiliated_base_number\"].iloc[np.where(pd_df[\"Affiliated_base_number\"].isna() == True)] = \"NaN\"\n"
     ]
    }
   ],
   "source": [
    "# so we replace them with the string \"NaN\"\n",
    "pd_df[\"Affiliated_base_number\"].iloc[np.where(pd_df[\"Affiliated_base_number\"].isna() == True)] = \"NaN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b80e2440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>SR_Flag</th>\n",
       "      <th>Affiliated_base_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B02764</td>\n",
       "      <td>2021-06-01 00:02:41</td>\n",
       "      <td>2021-06-01 00:07:46</td>\n",
       "      <td>174</td>\n",
       "      <td>18</td>\n",
       "      <td>N</td>\n",
       "      <td>B02764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B02764</td>\n",
       "      <td>2021-06-01 00:16:16</td>\n",
       "      <td>2021-06-01 00:21:14</td>\n",
       "      <td>32</td>\n",
       "      <td>254</td>\n",
       "      <td>N</td>\n",
       "      <td>B02764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B02764</td>\n",
       "      <td>2021-06-01 00:27:01</td>\n",
       "      <td>2021-06-01 00:42:11</td>\n",
       "      <td>240</td>\n",
       "      <td>127</td>\n",
       "      <td>N</td>\n",
       "      <td>B02764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B02764</td>\n",
       "      <td>2021-06-01 00:46:08</td>\n",
       "      <td>2021-06-01 00:53:45</td>\n",
       "      <td>127</td>\n",
       "      <td>235</td>\n",
       "      <td>N</td>\n",
       "      <td>B02764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B02510</td>\n",
       "      <td>2021-06-01 00:45:42</td>\n",
       "      <td>2021-06-01 01:03:33</td>\n",
       "      <td>144</td>\n",
       "      <td>146</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dispatching_base_num      pickup_datetime     dropoff_datetime  \\\n",
       "0               B02764  2021-06-01 00:02:41  2021-06-01 00:07:46   \n",
       "1               B02764  2021-06-01 00:16:16  2021-06-01 00:21:14   \n",
       "2               B02764  2021-06-01 00:27:01  2021-06-01 00:42:11   \n",
       "3               B02764  2021-06-01 00:46:08  2021-06-01 00:53:45   \n",
       "4               B02510  2021-06-01 00:45:42  2021-06-01 01:03:33   \n",
       "\n",
       "   PULocationID  DOLocationID SR_Flag Affiliated_base_number  \n",
       "0           174            18       N                 B02764  \n",
       "1            32           254       N                 B02764  \n",
       "2           240           127       N                 B02764  \n",
       "3           127           235       N                 B02764  \n",
       "4           144           146       N                    NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cf8654e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jagadish/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/home/jagadish/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropoff_datetime', StringType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('SR_Flag', StringType(), True), StructField('Affiliated_base_number', StringType(), True)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_session.createDataFrame(pd_df).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76d9896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy the above schema into some editor eg:vscode and add some spark.sql.types like below\n",
    "from pyspark.sql import types\n",
    "\n",
    "fhvhv_schema = types.StructType([types.StructField('dispatching_base_num', types.StringType(), True),\n",
    " types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    " types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    " types.StructField('PULocationID', types.IntegerType(), True),\n",
    " types.StructField('DOLocationID', types.IntegerType(), True),\n",
    " types.StructField('SR_Flag', types.StringType(), True),\n",
    " types.StructField('Affiliated_base_number', types.StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b70a2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = f'../notebooks/data/raw/fhvhv/2021/06/'\n",
    "output_path = f'../notebooks/data/pq/fhvhv/2021/06/'\n",
    "\n",
    "df_fhvhv = spark_session.read.option('header', 'true').schema(fhvhv_schema).csv(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6cc7a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# as asked in the Q2\n",
    "#Repartition it to 12 partitions and save it to parquet.\n",
    "df_fhvhv.repartition(12).write.parquet(output_path, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37c98323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 290748\r\n",
      "-rw-r--r-- 1 jagadish jagadish        0 Mar 16 17:09 _SUCCESS\r\n",
      "-rw-r--r-- 1 jagadish jagadish 24801840 Mar 16 17:08 part-00000-6f96884d-3885-4554-8c89-38f61d8a9890-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 jagadish jagadish 24799764 Mar 16 17:08 part-00001-6f96884d-3885-4554-8c89-38f61d8a9890-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 jagadish jagadish 24813665 Mar 16 17:08 part-00002-6f96884d-3885-4554-8c89-38f61d8a9890-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 jagadish jagadish 24816657 Mar 16 17:08 part-00003-6f96884d-3885-4554-8c89-38f61d8a9890-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 jagadish jagadish 24807133 Mar 16 17:09 part-00004-6f96884d-3885-4554-8c89-38f61d8a9890-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 jagadish jagadish 24800815 Mar 16 17:09 part-00005-6f96884d-3885-4554-8c89-38f61d8a9890-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 jagadish jagadish 24812906 Mar 16 17:09 part-00006-6f96884d-3885-4554-8c89-38f61d8a9890-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 jagadish jagadish 24809398 Mar 16 17:09 part-00007-6f96884d-3885-4554-8c89-38f61d8a9890-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 jagadish jagadish 24814315 Mar 16 17:09 part-00008-6f96884d-3885-4554-8c89-38f61d8a9890-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 jagadish jagadish 24810739 Mar 16 17:09 part-00009-6f96884d-3885-4554-8c89-38f61d8a9890-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 jagadish jagadish 24810799 Mar 16 17:09 part-00010-6f96884d-3885-4554-8c89-38f61d8a9890-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 jagadish jagadish 24803713 Mar 16 17:09 part-00011-6f96884d-3885-4554-8c89-38f61d8a9890-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "ls ../notebooks/data/pq/fhvhv/2021/06/ -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33faf7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg MB size of these partitions are 25MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352f613c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "149d5445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 6, 1, 0, 2, 41), dropoff_datetime=datetime.datetime(2021, 6, 1, 0, 7, 46), PULocationID=174, DOLocationID=18, SR_Flag='N', Affiliated_base_number='B02764'),\n",
       " Row(dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 6, 1, 0, 16, 16), dropoff_datetime=datetime.datetime(2021, 6, 1, 0, 21, 14), PULocationID=32, DOLocationID=254, SR_Flag='N', Affiliated_base_number='B02764'),\n",
       " Row(dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 6, 1, 0, 27, 1), dropoff_datetime=datetime.datetime(2021, 6, 1, 0, 42, 11), PULocationID=240, DOLocationID=127, SR_Flag='N', Affiliated_base_number='B02764'),\n",
       " Row(dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 6, 1, 0, 46, 8), dropoff_datetime=datetime.datetime(2021, 6, 1, 0, 53, 45), PULocationID=127, DOLocationID=235, SR_Flag='N', Affiliated_base_number='B02764'),\n",
       " Row(dispatching_base_num='B02510', pickup_datetime=datetime.datetime(2021, 6, 1, 0, 45, 42), dropoff_datetime=datetime.datetime(2021, 6, 1, 1, 3, 33), PULocationID=144, DOLocationID=146, SR_Flag='N', Affiliated_base_number=None)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3\n",
    "df_fhvhv.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff74d9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', TimestampType(), True), StructField('dropoff_datetime', TimestampType(), True), StructField('PULocationID', IntegerType(), True), StructField('DOLocationID', IntegerType(), True), StructField('SR_Flag', StringType(), True), StructField('Affiliated_base_number', StringType(), True)])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fhvhv.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d87c748",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pq_fhvhv = spark_session.read.parquet('../notebooks/data/pq/fhvhv/2021/06/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33b0e0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B02889|2021-06-04 20:51:44|2021-06-04 21:10:12|         239|         158|      N|                B02889|\n",
      "|              B02800|2021-06-04 15:50:15|2021-06-04 16:19:29|          75|         116|      N|                  null|\n",
      "|              B02510|2021-06-02 21:03:38|2021-06-02 21:10:12|         167|         168|      N|                  null|\n",
      "|              B02867|2021-06-02 12:51:57|2021-06-02 13:05:09|         151|         142|      N|                B02867|\n",
      "|              B02869|2021-06-21 09:51:45|2021-06-21 10:09:17|         106|          65|      N|                B02869|\n",
      "|              B02764|2021-06-02 13:27:03|2021-06-02 13:38:20|         113|         148|      N|                B02764|\n",
      "|              B02764|2021-06-10 14:48:23|2021-06-10 16:06:10|         250|         239|      N|                B02764|\n",
      "|              B02510|2021-06-09 22:38:49|2021-06-09 23:11:58|         132|          33|      N|                  null|\n",
      "|              B02510|2021-06-26 06:50:43|2021-06-26 07:00:20|         238|         244|      N|                  null|\n",
      "|              B02872|2021-06-07 08:04:00|2021-06-07 08:40:25|         198|         234|      N|                B02872|\n",
      "|              B02765|2021-06-24 20:19:42|2021-06-24 20:38:22|         147|          42|      N|                B02765|\n",
      "|              B02884|2021-06-05 17:38:24|2021-06-05 18:05:30|         142|         232|      N|                B02884|\n",
      "|              B02765|2021-06-13 15:44:27|2021-06-13 16:04:38|          26|         165|      N|                B02765|\n",
      "|              B02510|2021-06-26 18:37:10|2021-06-26 18:56:01|         237|         145|      N|                  null|\n",
      "|              B02764|2021-06-23 14:44:10|2021-06-23 14:58:06|          45|          33|      N|                B02764|\n",
      "|              B02510|2021-06-20 02:11:32|2021-06-20 02:22:00|          49|          37|      N|                  null|\n",
      "|              B02764|2021-06-29 19:02:50|2021-06-29 19:05:25|         140|         140|      N|                B02764|\n",
      "|              B02875|2021-06-02 15:27:05|2021-06-02 15:37:27|          32|         185|      N|                B02875|\n",
      "|              B02882|2021-06-18 12:48:43|2021-06-18 12:58:22|          95|         134|      N|                B02882|\n",
      "|              B02510|2021-06-23 08:59:54|2021-06-23 09:44:49|         171|         140|      N|                  null|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pq_fhvhv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88724371",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = df_pq_fhvhv.select('dispatching_base_num') \\\n",
    "    .filter((df_pq_fhvhv.pickup_datetime >= '2021-06-15 00:00:00') & (df_pq_fhvhv.pickup_datetime < '2021-06-16 00:00:00'))\n",
    "# OR the below query also works\n",
    "# df_results = df_fhvhv.select('dispatching_base_num') \\\n",
    "#     .filter((df_fhvhv.pickup_datetime >= '2021-06-15 00:00:00') & (df_fhvhv.pickup_datetime < '2021-06-16 00:00:00'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a0ac61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "452470"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925c3ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33a7dbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4\n",
    "def calculate_trip_duration(pickup_time, dropoff_time):\n",
    "#     print(dropoff_time, pickup_time)\n",
    "#     print(dropoff_time-pickup_time)\n",
    "    diff = (dropoff_time-pickup_time)\n",
    "    total_seconds = diff.total_seconds() # Convert timedelta into seconds\n",
    "#     print(total_seconds)\n",
    "    seconds_in_hour = 3600   # Set the number of seconds in an hour\n",
    "    td_in_hours = total_seconds / seconds_in_hour # Convert timedelta into hours\n",
    "#     print(td_in_hours)\n",
    "    return td_in_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad1d00d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types\n",
    "\n",
    "# user defined function\n",
    "calc_trip_duration = F.udf(calculate_trip_duration, returnType=types.StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7abfb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|dispatching_base_num|    trip_duration|\n",
      "+--------------------+-----------------+\n",
      "|              B02882|9.966388888888888|\n",
      "|              B02510|9.966388888888888|\n",
      "|              B02510|9.637777777777778|\n",
      "|              B02510|9.624444444444444|\n",
      "|              B02510|9.480277777777777|\n",
      "|              B02800|9.471666666666666|\n",
      "|              B02510|9.402222222222223|\n",
      "|              B02510|9.393611111111111|\n",
      "|              B02764|9.376944444444444|\n",
      "|              B02617|9.365833333333333|\n",
      "|              B02510|9.246666666666666|\n",
      "|              B02887|9.106944444444444|\n",
      "|              B02510|9.056111111111111|\n",
      "|              B02510|9.030277777777778|\n",
      "|              B02510|8.817777777777778|\n",
      "|              B02879|8.774166666666666|\n",
      "|              B02510|8.758333333333333|\n",
      "|              B02510|           8.6875|\n",
      "|              B02510|8.571666666666667|\n",
      "|              B02510|8.559444444444445|\n",
      "+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_pq_fhvhv.withColumn('trip_duration', calc_trip_duration(F.to_timestamp(df_pq_fhvhv.pickup_datetime), F.to_timestamp(df_pq_fhvhv.dropoff_datetime))) \\\n",
    "    .select('dispatching_base_num', 'trip_duration') \\\n",
    "    .orderBy(F.col('trip_duration').desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "968319ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the above UDF & its custom function are working fine but some logic is missing, so we'll go with the spark inbuilt functions only\n",
    "# Also, either the spark with SQL or spark with inbuilt functions are better when compared to the spark with UDF because they're costly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "061de07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 15:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------------+\n",
      "|   dropoff_datetime|    pickup_datetime|        diff_hours|\n",
      "+-------------------+-------------------+------------------+\n",
      "|2021-06-28 08:48:25|2021-06-25 13:55:41|  66.8788888888889|\n",
      "|2021-06-23 13:42:44|2021-06-22 12:09:45|25.549722222222222|\n",
      "|2021-06-28 06:31:20|2021-06-27 10:32:29|19.980833333333333|\n",
      "|2021-06-27 16:49:01|2021-06-26 22:37:11|18.197222222222223|\n",
      "|2021-06-24 13:08:44|2021-06-23 20:40:43|16.466944444444444|\n",
      "|2021-06-24 12:19:39|2021-06-23 22:03:31|14.268888888888888|\n",
      "|2021-06-25 13:05:35|2021-06-24 23:11:00|13.909722222222221|\n",
      "|2021-06-05 08:36:14|2021-06-04 20:56:02|             11.67|\n",
      "|2021-06-27 19:07:16|2021-06-27 07:45:19|11.365833333333333|\n",
      "|2021-06-21 04:04:16|2021-06-20 17:05:12|10.984444444444444|\n",
      "+-------------------+-------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_pq_fhvhv.select(\n",
    "                F.col(\"dropoff_datetime\"), F.col(\"pickup_datetime\")) \\\n",
    "                .withColumn(\"diff_hours\", (F.col(\"dropoff_datetime\").cast(\"long\") - F.col(\"pickup_datetime\").cast(\"long\"))/3600) \\\n",
    "                .orderBy(\"diff_hours\", ascending=False) \\\n",
    "                .limit(10) \\\n",
    "                .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7394526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa778d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc678bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3eb37bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B02889|2021-06-04 20:51:44|2021-06-04 21:10:12|         239|         158|      N|                B02889|\n",
      "|              B02800|2021-06-04 15:50:15|2021-06-04 16:19:29|          75|         116|      N|                  null|\n",
      "|              B02510|2021-06-02 21:03:38|2021-06-02 21:10:12|         167|         168|      N|                  null|\n",
      "|              B02867|2021-06-02 12:51:57|2021-06-02 13:05:09|         151|         142|      N|                B02867|\n",
      "|              B02869|2021-06-21 09:51:45|2021-06-21 10:09:17|         106|          65|      N|                B02869|\n",
      "|              B02764|2021-06-02 13:27:03|2021-06-02 13:38:20|         113|         148|      N|                B02764|\n",
      "|              B02764|2021-06-10 14:48:23|2021-06-10 16:06:10|         250|         239|      N|                B02764|\n",
      "|              B02510|2021-06-09 22:38:49|2021-06-09 23:11:58|         132|          33|      N|                  null|\n",
      "|              B02510|2021-06-26 06:50:43|2021-06-26 07:00:20|         238|         244|      N|                  null|\n",
      "|              B02872|2021-06-07 08:04:00|2021-06-07 08:40:25|         198|         234|      N|                B02872|\n",
      "|              B02765|2021-06-24 20:19:42|2021-06-24 20:38:22|         147|          42|      N|                B02765|\n",
      "|              B02884|2021-06-05 17:38:24|2021-06-05 18:05:30|         142|         232|      N|                B02884|\n",
      "|              B02765|2021-06-13 15:44:27|2021-06-13 16:04:38|          26|         165|      N|                B02765|\n",
      "|              B02510|2021-06-26 18:37:10|2021-06-26 18:56:01|         237|         145|      N|                  null|\n",
      "|              B02764|2021-06-23 14:44:10|2021-06-23 14:58:06|          45|          33|      N|                B02764|\n",
      "|              B02510|2021-06-20 02:11:32|2021-06-20 02:22:00|          49|          37|      N|                  null|\n",
      "|              B02764|2021-06-29 19:02:50|2021-06-29 19:05:25|         140|         140|      N|                B02764|\n",
      "|              B02875|2021-06-02 15:27:05|2021-06-02 15:37:27|          32|         185|      N|                B02875|\n",
      "|              B02882|2021-06-18 12:48:43|2021-06-18 12:58:22|          95|         134|      N|                B02882|\n",
      "|              B02510|2021-06-23 08:59:54|2021-06-23 09:44:49|         171|         140|      N|                  null|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q6\n",
    "df_pq_fhvhv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0cbf2910",
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_df = pd.read_csv('https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20190b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LocationID</th>\n",
       "      <th>Borough</th>\n",
       "      <th>Zone</th>\n",
       "      <th>service_zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>EWR</td>\n",
       "      <td>Newark Airport</td>\n",
       "      <td>EWR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Jamaica Bay</td>\n",
       "      <td>Boro Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Allerton/Pelham Gardens</td>\n",
       "      <td>Boro Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Alphabet City</td>\n",
       "      <td>Yellow Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>Arden Heights</td>\n",
       "      <td>Boro Zone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LocationID        Borough                     Zone service_zone\n",
       "0           1            EWR           Newark Airport          EWR\n",
       "1           2         Queens              Jamaica Bay    Boro Zone\n",
       "2           3          Bronx  Allerton/Pelham Gardens    Boro Zone\n",
       "3           4      Manhattan            Alphabet City  Yellow Zone\n",
       "4           5  Staten Island            Arden Heights    Boro Zone"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zones_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9042d2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 - mkdir -p data/raw/zones\n",
    "#2 - wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv -O data/raw/zones/taxi_zone_lookup.csv\n",
    "\n",
    "df_zones = spark_session.read.option('header', 'true').csv('../notebooks/data/raw/zones/taxi_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f301f2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zones.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "45b51660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jagadish/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df_pq_fhvhv.registerTempTable('fhvhv_2021_06')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "30a4c285",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones.registerTempTable('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "61b053b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = spark_session.sql(\"\"\"\n",
    "select *\n",
    "from fhvhv_2021_06 as fhvhv\n",
    "join zones on fhvhv.PULocationID=zones.LocationID\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f41c19e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14961892"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81f429e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zones.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bf8cef93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14961892"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pq_fhvhv.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c852aba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = spark_session.sql(\"\"\"\n",
    "select zones.LocationID, count(1) as times_picked, zones.Zone\n",
    "from fhvhv_2021_06 as fhvhv\n",
    "join zones on fhvhv.PULocationID=zones.LocationID\n",
    "group by zones.LocationID, zones.Zone\n",
    "order by times_picked desc\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4342351b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 31:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+\n",
      "|LocationID|times_picked|                Zone|\n",
      "+----------+------------+--------------------+\n",
      "|        61|      231279| Crown Heights North|\n",
      "|        79|      221244|        East Village|\n",
      "|       132|      188867|         JFK Airport|\n",
      "|        37|      187929|      Bushwick South|\n",
      "|        76|      186780|       East New York|\n",
      "|       231|      164344|TriBeCa/Civic Center|\n",
      "|       138|      161596|   LaGuardia Airport|\n",
      "|       234|      158937|            Union Sq|\n",
      "|       249|      154698|        West Village|\n",
      "|         7|      152493|             Astoria|\n",
      "|       148|      151020|     Lower East Side|\n",
      "|        68|      147673|        East Chelsea|\n",
      "|        42|      146402|Central Harlem North|\n",
      "|       255|      143683|Williamsburg (Nor...|\n",
      "|       181|      143594|          Park Slope|\n",
      "|       225|      141427|  Stuyvesant Heights|\n",
      "|        48|      139611|        Clinton East|\n",
      "|       246|      139431|West Chelsea/Huds...|\n",
      "|        17|      138428|             Bedford|\n",
      "|       170|      137879|         Murray Hill|\n",
      "+----------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b793607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
